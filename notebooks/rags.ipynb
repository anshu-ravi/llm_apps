{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = 'data/Segment-anything-meta-paper.pdf'\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "data = loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='(c) Data: data engine (top) & dataset (bottom)•1+ billion masks•11 million images •privacy respecting•licensed imagesannotatetraindatamodelSegment Anything 1B (SA-1B):\\nFigure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-\\nable segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range', metadata={'source': 'data/Segment-anything-meta-paper.pdf', 'page': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To index our pdf, we use an open source tool called Faiss.\n",
    "# it is a library for efficient similarity search and clustering of dense vectors.\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# An example of how to embed a query using the OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector1 = embeddings.embed_query('How are you?')\n",
    "len(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Results. We show qualitative results in Fig. 12. SAM\\ncan segment objects based on simple text prompts like “a\\nwheel” as well as phrases like “beaver tooth grille”. When\\nSAM fails to pick the right object from a text prompt only,\\nan additional point often fixes the prediction, similar to [31].\\n7.6. Ablations\\nWe perform several ablations on our 23 dataset suite with\\nthe single center point prompt protocol. Recall that a sin-\\ngle point may be ambiguous and that ambiguity may not', metadata={'source': 'data/Segment-anything-meta-paper.pdf', 'page': 10}),\n",
       "  0.6708498665022007),\n",
       " (Document(page_content='that annotators improved annotation quality and quantity over time.\\n3.How did you choose the specific wording of your task instructions? What\\nsteps, if any, were taken to verify the clarity of task instructions and wording\\nfor annotators? As our task was annotating images, the annotation guide-\\nlines included visual examples. Our research team completed 30 annotation\\ntasks to identify any obvious challenges using the annotation tool, collec-', metadata={'source': 'data/Segment-anything-meta-paper.pdf', 'page': 26}),\n",
       "  0.6699014991904328),\n",
       " (Document(page_content='manually reviewed the annotations weekly, sharing feedback weekly.\\n2.Have you conducted any analysis on disagreement patterns? If so, what\\nanalyses did you use and what were the major findings? Did you analyze\\npotential sources of disagreement? We pointed out common mistakes dur-\\ning weekly meetings with the annotators.\\n3.How do the individual annotator responses relate to the final labels released\\nin the dataset? The annotations were only used to train early versions of the', metadata={'source': 'data/Segment-anything-meta-paper.pdf', 'page': 26}),\n",
       "  0.6690653478628252),\n",
       " (Document(page_content='The success of this plan hinges on three components:\\ntask,model , and data . To develop them, we address the\\nfollowing questions about image segmentation:\\n1. What task will enable zero-shot generalization?\\n2. What is the corresponding model architecture?\\n3. What data can power this task and model?\\n1', metadata={'source': 'data/Segment-anything-meta-paper.pdf', 'page': 0}),\n",
       "  0.6681230075503943)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "index = FAISS.from_documents(data, embeddings)\n",
    "\n",
    "question = \"Explain the main idea and results of the paper.\"\n",
    "index.similarity_search_with_relevance_scores(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Results. We show qualitative results in Fig. 12. SAM\n",
      "can segment objects based on simple text prompts like “a\n",
      "wheel” as well as phrases like “beaver tooth grille”. When\n",
      "SAM fails to pick the right object from a text prompt only,\n",
      "an additional point often fixes the prediction, similar to [31].\n",
      "7.6. Ablations\n",
      "We perform several ablations on our 23 dataset suite with\n",
      "the single center point prompt protocol. Recall that a sin-\n",
      "gle point may be ambiguous and that ambiguity may not\n",
      "\n",
      "that annotators improved annotation quality and quantity over time.\n",
      "3.How did you choose the specific wording of your task instructions? What\n",
      "steps, if any, were taken to verify the clarity of task instructions and wording\n",
      "for annotators? As our task was annotating images, the annotation guide-\n",
      "lines included visual examples. Our research team completed 30 annotation\n",
      "tasks to identify any obvious challenges using the annotation tool, collec-\n",
      "\n",
      "manually reviewed the annotations weekly, sharing feedback weekly.\n",
      "2.Have you conducted any analysis on disagreement patterns? If so, what\n",
      "analyses did you use and what were the major findings? Did you analyze\n",
      "potential sources of disagreement? We pointed out common mistakes dur-\n",
      "ing weekly meetings with the annotators.\n",
      "3.How do the individual annotator responses relate to the final labels released\n",
      "in the dataset? The annotations were only used to train early versions of the\n",
      "\n",
      "The success of this plan hinges on three components:\n",
      "task,model , and data . To develop them, we address the\n",
      "following questions about image segmentation:\n",
      "1. What task will enable zero-shot generalization?\n",
      "2. What is the corresponding model architecture?\n",
      "3. What data can power this task and model?\n",
      "1\n",
      "\n",
      "that the nearest neighbors for each query show some, albeit\n",
      "imperfect, shape and semantic similarity. Although these\n",
      "results are preliminary, they indicate that the representations\n",
      "from SAM may be useful for a variety of purposes, such as\n",
      "further data labeling, understanding the contents of datasets,\n",
      "or as features for downstream tasks.\n",
      "E. Human Study Experimental Design\n",
      "Here we describe details of the human study used to eval-\n",
      "uate mask quality in §7.1 and §7.4. The purpose of the\n",
      "\n",
      "it possible to train the annotators providing feedback and answering their\n",
      "questions on a regular basis. Specifically: (1) By giving a clear understand-\n",
      "ing of the goals of this work and providing clear guidelines, including vi-\n",
      "suals and video recordings of the tasks, annotators had enough context to\n",
      "understand and perform the tasks reasonably. (2) Sharing objectives and\n",
      "key results and meeting weekly with annotators increased the likelihood\n",
      "\n",
      "Responsible AI (§6). We study and report on potential fair-\n",
      "ness concerns and biases when using SA-1B and SAM. Im-\n",
      "ages in SA-1B span a geographically and economically di-\n",
      "verse set of countries and we found that SAM performs sim-\n",
      "ilarly across different groups of people. Together, we hope\n",
      "this will make our work more equitable for real-world use\n",
      "cases. We provide model and dataset cards in the appendix.\n",
      "Experiments (§7). We extensively evaluate SAM. First, us-\n",
      "\n",
      "vary in their date taken over a wide range of years up to 2022.\n",
      "6.Were any ethical review processes conducted ( e.g., by an institutional re-\n",
      "view board)? If so, please provide a description of these review processes,\n",
      "including the outcomes, as well as a link or other access point to any sup-\n",
      "porting documentation. If the dataset does not relate to people, you may skip\n",
      "the remaining questions in this section. We underwent an internal privacy\n",
      "\n",
      "tively decide how to handle complex cases, and refine the guidelines. The\n",
      "research team met with the annotators weekly for feedback sessions. Videos\n",
      "of the research team performing the task were shared live with the annota-\n",
      "tors, followed by Q&A sessions. Annotators were able to give feedback on\n",
      "unclear aspects, both during the feedback session and asynchronously.\n",
      "4.What, if any, risks did your task pose for annotators and were they informed\n",
      "\n",
      "mind? Was there a specific gap that needed to be filled? Please provide a\n",
      "description. The contributions of our dataset to the vision community are\n",
      "fourfold: (1) We release a dataset of 11M images and 1.1B masks, by far the\n",
      "largest segmentation dataset to date. (2) The dataset we release is privacy\n",
      "protecting: we have blurred faces and license plates in all images. (3) The\n",
      "dataset is licensed under a broad set of terms of use which can be found\n",
      "Human: Explain the main idea and results of the paper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main idea of the paper is the development and evaluation of SAM, a model for segmenting objects in images based on text prompts. SAM can accurately segment objects based on simple text prompts like \"a wheel\" or more complex phrases like \"beaver tooth grille.\" The paper discusses the success of SAM in accurately segmenting objects and how additional points can often fix any incorrect predictions. Additionally, the paper discusses ablations performed on a dataset suite and the importance of task, model, and data components in image segmentation.\\n\\nIn terms of results, the paper shows qualitative results in Figure 12, demonstrating SAM\\'s capabilities in segmenting objects based on text prompts. The paper also mentions that the representations from SAM may be useful for various purposes, such as data labeling, understanding dataset contents, or as features for downstream tasks. Furthermore, the paper discusses a human study experimental design used to evaluate mask quality, as well as fairness concerns and biases when using SAM across diverse groups of people.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "retriever.search_kwargs['fetch_k'] = 20\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 10\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "chain.run(\n",
    "    query=question,\n",
    "    callbacks=[handler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data into a Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone \n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pc_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc_env = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "pc = pinecone.Pinecone(api_key=pc_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_name = 'demo'\n",
    "db = Pinecone.from_documents(\n",
    "    data, \n",
    "    embeddings, \n",
    "    index_name=proj_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain the main idea and results of the paper.',\n",
       " 'result': 'The main idea of the paper is the development of a system called SAM, which can segment objects in images based on text prompts. The system shows promising results in segmenting objects based on simple text prompts like \"a wheel\" or more complex phrases like \"beaver tooth grille.\" Additionally, when SAM fails to pick the right object from a text prompt initially, an additional point often helps correct the prediction.\\n\\nThe results of the paper demonstrate the effectiveness of SAM in object segmentation based on text prompts, showcasing its ability to understand and segment objects accurately in images. The system\\'s performance is highlighted through qualitative results shown in Figure 12, indicating its capability to interpret and act on textual instructions for image segmentation tasks.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=db.as_retriever(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.invoke(input = question, \n",
    "             callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
