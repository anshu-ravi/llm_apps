{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "AUDIENCE = \"general\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Creating a GPT model for all purposes\n",
    "# Will change behaviour of the specific instance using the prompt\n",
    "llm = ChatOpenAI(temperature=0.5, max_tokens=700, model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167014\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "file_path = \"../notebooks/data/Segment-anything-meta-paper.pdf\"\n",
    "pdfreader = PdfReader(file_path)\n",
    "\n",
    "# Read text from the pdf \n",
    "text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        text += content\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' And thank you to our listeners for tuning in. Stay tuned for more exciting episodes where we explore the latest innovations in the world of technology. Until next time!'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42020\n"
     ]
    }
   ],
   "source": [
    "# Total number of input tokens in the document \n",
    "print(llm.get_num_tokens(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our context window is only 16K, we need to split our document into multiple smaller chunks, process them and them combine the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 14\n"
     ]
    }
   ],
   "source": [
    "# Splitting the text \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=12000, chunk_overlap=50)\n",
    "chunks = splitter.create_documents([text])\n",
    "print(f'Number of chunks: {len(chunks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Segment Anything\\nAlexander Kirillov1,2,4Eric Mintun2Nikhila Ravi1,2Hanzi Mao2Chloe Rolland3Laura Gustafson3\\nTete Xiao3Spencer Whitehead Alexander C. Berg Wan-Yen Lo Piotr Doll ´ar4Ross Girshick4\\n1project lead2joint first author3equal contribution4directional lead\\nMeta AI Research, FAIR\\n(b) Model: Segment Anything Model (SAM)promptimagevalid maskimage encoderprompt encoderlightweight mask decoder\\n(a) Task: promptable segmentationsegmentation promptimagemodelcat withblack earsvalid mask\\n(c) Data: data engine (top) & dataset (bottom)•1+ billion masks•11 million images •privacy respecting•licensed imagesannotatetraindatamodelSegment Anything 1B (SA-1B):\\nFigure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-\\nable segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range\\nof tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.\\nAbstract\\nWe introduce the Segment Anything (SA) project: a new\\ntask, model, and dataset for image segmentation. Using our\\nefficient model in a data collection loop, we built the largest\\nsegmentation dataset to date (by far), with over 1 billion\\nmasks on 11M licensed and privacy respecting images. The\\nmodel is designed and trained to be promptable, so it can\\ntransfer zero-shot to new image distributions and tasks. We\\nevaluate its capabilities on numerous tasks and find that\\nits zero-shot performance is impressive – often competitive\\nwith or even superior to prior fully supervised results. We\\nare releasing the Segment Anything Model (SAM) and cor-\\nresponding dataset (SA-1B) of 1B masks and 11M images at\\nhttps://segment-anything.com to foster research into foun-\\ndation models for computer vision.\\n1. Introduction\\nLarge language models pre-trained on web-scale datasets\\nare revolutionizing NLP with strong zero-shot and few-shot\\ngeneralization [10]. These “foundation models” [8] can\\ngeneralize to tasks and data distributions beyond those seen\\nduring training. This capability is often implemented with\\nprompt engineering in which hand-crafted text is used to\\nprompt the language model to generate a valid textual re-\\nsponse for the task at hand. When scaled and trained with\\nabundant text corpora from the web, these models’ zero and\\nfew-shot performance compares surprisingly well to (evenmatching in some cases) fine-tuned models [10, 21]. Empir-\\nical trends show this behavior improving with model scale,\\ndataset size, and total training compute [56, 10, 21, 51].\\nFoundation models have also been explored in computer\\nvision, albeit to a lesser extent. Perhaps the most promi-\\nnent illustration aligns paired text and images from the web.\\nFor example, CLIP [82] and ALIGN [55] use contrastive\\nlearning to train text and image encoders that align the two\\nmodalities. Once trained, engineered text prompts enable\\nzero-shot generalization to novel visual concepts and data\\ndistributions. Such encoders also compose effectively with\\nother modules to enable downstream tasks, such as image\\ngeneration ( e.g., DALL·E [83]). While much progress has\\nbeen made on vision and language encoders, computer vi-\\nsion includes a wide range of problems beyond this scope,\\nand for many of these, abundant training data does not exist.\\nIn this work, our goal is to build a foundation model for\\nimage segmentation . That is, we seek to develop a prompt-\\nable model and pre-train it on a broad dataset using a task\\nthat enables powerful generalization. With this model, we\\naim to solve a range of downstream segmentation problems\\non new data distributions using prompt engineering.\\nThe success of this plan hinges on three components:\\ntask,model , and data . To develop them, we address the\\nfollowing questions about image segmentation:\\n1. What task will enable zero-shot generalization?\\n2. What is the corresponding model architecture?\\n3. What data can power this task and model?\\n1These questions are entangled and require a comprehen-\\nsive solution. We start by defining a promptable segmenta-\\ntiontask that is general enough to provide a powerful pre-\\ntraining objective and to enable a wide range of downstream\\napplications. This task requires a model that supports flex-\\nible prompting and can output segmentation masks in real-\\ntime when prompted to allow for interactive use. To train\\nour model, we need a diverse, large-scale source of data .\\nUnfortunately, there is no web-scale data source for seg-\\nmentation; to address this, we build a “data engine”, i.e.,\\nwe iterate between using our efficient model to assist in data\\ncollection and using the newly collected data to improve the\\nmodel. We introduce each interconnected component next,\\nfollowed by the dataset we created and the experiments that\\ndemonstrate the effectiveness of our approach.\\nTask (§2). In NLP and more recently computer vision,\\nfoundation models are a promising development that can\\nperform zero-shot and few-shot learning for new datasets\\nand tasks often by using “prompting” techniques. Inspired\\nby this line of work, we propose the promptable segmen-\\ntation task , where the goal is to return a valid segmenta-\\ntion mask given any segmentation prompt (see Fig. 1a). A\\nprompt simply specifies what to segment in an image, e.g.,\\na prompt can include spatial or text information identifying\\nan object. The requirement of a valid output mask means\\nthat even when a prompt is ambiguous and could refer to\\nmultiple objects (for example, a point on a shirt may in-\\ndicate either the shirt or the person wearing it), the output\\nshould be a reasonable mask for at least one of those ob-\\njects. We use the promptable segmentation task as both a\\npre-training objective and to solve general downstream seg-\\nmentation tasks via prompt engineering.\\nModel (§3). The promptable segmentation task and the goal\\nof real-world use impose constraints on the model architec-\\nture. In particular, the model must support flexible prompts ,\\nneeds to compute masks in amortized real-time to allow in-\\nteractive use, and must be ambiguity-aware . Surprisingly,\\nwe find that a simple design satisfies all three constraints:\\na powerful image encoder computes an image embedding,\\na prompt encoder embeds prompts, and then the two infor-\\nmation sources are combined in a lightweight mask decoder\\nthat predicts segmentation masks. We refer to this model as\\nthe Segment Anything Model, or SAM (see Fig. 1b). By\\nseparating SAM into an image encoder and a fast prompt\\nencoder / mask decoder, the same image embedding can\\nbe reused (and its cost amortized) with different prompts.\\nGiven an image embedding, the prompt encoder and mask\\ndecoder predict a mask from a prompt in ∼50ms in a web\\nbrowser. We focus on point, box, and mask prompts, and\\nalso present initial results with free-form text prompts. To\\nmake SAM ambiguity-aware, we design it to predict mul-\\ntiple masks for a single prompt allowing SAM to naturally\\nhandle ambiguity, such as the shirt vs. person example.Data engine (§4). To achieve strong generalization to new\\ndata distributions, we found it necessary to train SAM on\\na large and diverse set of masks, beyond any segmenta-\\ntion dataset that already exists. While a typical approach\\nfor foundation models is to obtain data online [82], masks\\nare not naturally abundant and thus we need an alternative\\nstrategy. Our solution is to build a “data engine”, i.e., we\\nco-develop our model with model-in-the-loop dataset an-\\nnotation (see Fig. 1c). Our data engine has three stages:\\nassisted-manual ,semi-automatic , and fully automatic . In\\nthe first stage, SAM assists annotators in annotating masks,\\nsimilar to a classic interactive segmentation setup. In the\\nsecond stage, SAM can automatically generate masks for\\na subset of objects by prompting it with likely object lo-\\ncations and annotators focus on annotating the remaining\\nobjects, helping increase mask diversity. In the final stage,\\nwe prompt SAM with a regular grid of foreground points,\\nyielding on average ∼100 high-quality masks per image.\\nDataset (§5). Our final dataset, SA-1B, includes more than\\n1Bmasks from 11M licensed and privacy-preserving im-\\nages (see Fig. 2). SA-1B, collected fully automatically us-\\ning the final stage of our data engine, has 400 ×more masks\\nthan any existing segmentation dataset [66, 44, 117, 60],\\nand as we verify extensively, the masks are of high quality\\nand diversity. Beyond its use in training SAM to be robust\\nand general, we hope SA-1B becomes a valuable resource\\nfor research aiming to build new foundation models.\\nResponsible AI (§6). We study and report on potential fair-\\nness concerns and biases when using SA-1B and SAM. Im-\\nages in SA-1B span a geographically and economically di-\\nverse set of countries and we found that SAM performs sim-\\nilarly across different groups of people. Together, we hope\\nthis will make our work more equitable for real-world use\\ncases. We provide model and dataset cards in the appendix.\\nExperiments (§7). We extensively evaluate SAM. First, us-\\ning a diverse new suite of 23 segmentation datasets, we find\\nthat SAM produces high-quality masks from a single fore-\\nground point, often only slightly below that of the manu-\\nally annotated ground truth. Second, we find consistently\\nstrong quantitative and qualitative results on a variety of\\ndownstream tasks under a zero-shot transfer protocol using\\nprompt engineering, including edge detection, object pro-\\nposal generation, instance segmentation, and a preliminary\\nexploration of text-to-mask prediction. These results sug-\\ngest that SAM can be used out-of-the-box with prompt en-\\ngineering to solve a variety of tasks involving object and\\nimage distributions beyond SAM’s training data. Neverthe-\\nless, room for improvement remains, as we discuss in §8.\\nRelease. We are releasing the SA-1B dataset for research\\npurposes and making SAM available under a permissive\\nopen license (Apache 2.0) at https://segment-anything.com.\\nWe also showcase SAM’s capabilities with an online demo.\\n2<50 masks\\n 50-100 masks\\n 100-200 masks\\n 200-300 masks\\n 300-400 masks\\n 400-500 masks\\n >500 masks\\nFigure 2: Example images with overlaid masks from our newly introduced dataset, SA-1B . SA-1B contains 11M diverse,\\nhigh-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were\\nannotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and\\ndiversity. We group images by number of masks per image for visualization (there are ∼100 masks per image on average).\\n32. Segment Anything Task\\nWe take inspiration from NLP, where the next token pre-\\ndiction task is used for foundation model pre-training and\\nto solve diverse downstream tasks via prompt engineer-\\ning [10]. To build a foundation model for segmentation,\\nwe aim to define a task with analogous capabilities.\\nTask. We start by translating the idea of a prompt from NLP\\nto segmentation, where a prompt can be a set of foreground\\n/ background points, a rough box or mask, free-form text,\\nor, in general, any information indicating what to segment\\nin an image. The promptable segmentation task , then, is to\\nreturn a valid segmentation mask given any prompt . The re-\\nquirement of a “valid” mask simply means that even when\\na prompt is ambiguous and could refer to multiple objects\\n(e.g., recall the shirt vs. person example, and see Fig. 3),\\nthe output should be a reasonable mask for at least oneof\\nthose objects. This requirement is similar to expecting a lan-\\nguage model to output a coherent response to an ambiguous\\nprompt. We choose this task because it leads to a natural\\npre-training algorithm anda general method for zero-shot\\ntransfer to downstream segmentation tasks via prompting.\\nPre-training. The promptable segmentation task suggests a\\nnatural pre-training algorithm that simulates a sequence of\\nprompts ( e.g., points, boxes, masks) for each training sam-'),\n",
       " Document(page_content='ple and compares the model’s mask predictions against the\\nground truth. We adapt this method from interactive seg-\\nmentation [109, 70], although unlike interactive segmenta-\\ntion whose aim is to eventually predict a valid mask after\\nenough user input, our aim is to always predict a valid mask\\nforany prompt even when the prompt is ambiguous . This\\nensures that a pre-trained model is effective in use cases that\\ninvolve ambiguity, including automatic annotation as re-\\nquired by our data engine §4. We note that performing well\\nat this task is challenging and requires specialized modeling\\nand training loss choices, which we discuss in §3.\\nZero-shot transfer. Intuitively, our pre-training task en-\\ndows the model with the ability to respond appropriately to\\nany prompt at inference time, and thus downstream tasks\\ncan be solved by engineering appropriate prompts. For ex-\\nample, if one has a bounding box detector for cats, cat in-\\nstance segmentation can be solved by providing the detec-\\ntor’s box output as a prompt to our model. In general, a wide\\narray of practical segmentation tasks can be cast as prompt-\\ning. In addition to automatic dataset labeling, we explore\\nfive diverse example tasks in our experiments in §7.\\nRelated tasks. Segmentation is a broad field: there’s in-\\nteractive segmentation [57, 109], edge detection [3], su-\\nper pixelization [85], object proposal generation [2], fore-\\nground segmentation [94], semantic segmentation [90], in-\\nstance segmentation [66], panoptic segmentation [59], etc.\\nThe goal of our promptable segmentation task is to produce\\nFigure 3: Each column shows 3 valid masks generated by\\nSAM from a single ambiguous point prompt (green circle).\\na broadly capable model that can adapt to many (though\\nnot all) existing and new segmentation tasks via prompt\\nengineering. This capability is a form of task generaliza-\\ntion [26]. Note that this is different than previous work on\\nmulti-task segmentation systems. In a multi-task system, a\\nsingle model performs a fixed set of tasks, e.g., joint seman-\\ntic, instance, and panoptic segmentation [114, 19, 54], but\\nthe training and test tasks are the same. An important dis-\\ntinction in our work is that a model trained for promptable\\nsegmentation can perform a new, different task at inference\\ntime by acting as a component in a larger system, e.g., to\\nperform instance segmentation, a promptable segmentation\\nmodel is combined with an existing object detector.\\nDiscussion. Prompting and composition are powerful tools\\nthat enable a single model to be used in extensible ways, po-\\ntentially to accomplish tasks unknown at the time of model\\ndesign. This approach is analogous to how other founda-\\ntion models are used, e.g., how CLIP [82] is the text-image\\nalignment component of the DALL ·E [83] image generation\\nsystem. We anticipate that composable system design, pow-\\nered by techniques such as prompt engineering, will enable\\na wider variety of applications than systems trained specif-\\nically for a fixed set of tasks. It’s also interesting to com-\\npare promptable and interactive segmentation through the\\nlens of composition: while interactive segmentation mod-\\nels are designed with human users in mind, a model trained\\nfor promptable segmentation can also be composed into a\\nlarger algorithmic system as we will demonstrate.\\n4,score\\nscore\\nscore,,\\nvalid masksimage\\nimage\\nencoder\\nimage\\nembeddingmask points box textprompt encodermask decoder\\nconvFigure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can\\nthen be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous\\nprompts corresponding to more than one object, SAM can output multiple valid masks and associated confidence scores.\\n3. Segment Anything Model\\nWe next describe the Segment Anything Model (SAM)\\nfor promptable segmentation. SAM has three components,\\nillustrated in Fig. 4: an image encoder, a flexible prompt\\nencoder, and a fast mask decoder. We build on Transformer\\nvision models [14, 33, 20, 62] with specific tradeoffs for\\n(amortized) real-time performance. We describe these com-\\nponents at a high-level here, with details in §A.\\nImage encoder. Motivated by scalability and powerful pre-\\ntraining methods, we use an MAE [47] pre-trained Vision\\nTransformer (ViT) [33] minimally adapted to process high\\nresolution inputs [62]. The image encoder runs once per\\nimage and can be applied prior to prompting the model.\\nPrompt encoder. We consider two sets of prompts: sparse\\n(points, boxes, text) and dense (masks). We represent\\npoints and boxes by positional encodings [95] summed with\\nlearned embeddings for each prompt type and free-form text\\nwith an off-the-shelf text encoder from CLIP [82]. Dense\\nprompts ( i.e., masks) are embedded using convolutions and\\nsummed element-wise with the image embedding.\\nMask decoder. The mask decoder efficiently maps the im-\\nage embedding, prompt embeddings, and an output token\\nto a mask. This design, inspired by [14, 20], employs a\\nmodification of a Transformer decoder block [103] followed\\nby a dynamic mask prediction head. Our modified decoder\\nblock uses prompt self-attention and cross-attention in two\\ndirections (prompt-to-image embedding and vice-versa) to\\nupdate allembeddings. After running two blocks, we up-\\nsample the image embedding and an MLP maps the output\\ntoken to a dynamic linear classifier, which then computes\\nthe mask foreground probability at each image location.\\nResolving ambiguity. With one output, the model will av-\\nerage multiple valid masks if given an ambiguous prompt.\\nTo address this, we modify the model to predict multiple\\noutput masks for a single prompt (see Fig. 3). We found\\n3 mask outputs is sufficient to address most common cases\\n(nested masks are often at most three deep: whole, part, and\\nsubpart). During training, we backprop only the minimumloss [15, 45, 64] over masks. To rank masks, the model pre-\\ndicts a confidence score ( i.e., estimated IoU) for each mask.\\nEfficiency. The overall model design is largely motivated\\nby efficiency. Given a precomputed image embedding, the\\nprompt encoder and mask decoder run in a web browser, on\\nCPU, in ∼50ms. This runtime performance enables seam-\\nless, real-time interactive prompting of our model.\\nLosses and training. We supervise mask prediction with\\nthe linear combination of focal loss [65] and dice loss [73]\\nused in [14]. We train for the promptable segmentation task\\nusing a mixture of geometric prompts (for text prompts see\\n§7.5). Following [92, 37], we simulate an interactive setup\\nby randomly sampling prompts in 11 rounds per mask, al-\\nlowing SAM to integrate seamlessly into our data engine.\\n4. Segment Anything Data Engine\\nAs segmentation masks are not abundant on the inter-\\nnet, we built a data engine to enable the collection of our\\n1.1B mask dataset, SA-1B. The data engine has three\\nstages: (1) a model-assisted manual annotation stage, (2) a\\nsemi-automatic stage with a mix of automatically predicted\\nmasks and model-assisted annotation, and (3) a fully auto-\\nmatic stage in which our model generates masks without\\nannotator input. We go into details of each next.\\nAssisted-manual stage. In the first stage, resembling clas-\\nsic interactive segmentation, a team of professional annota-\\ntors labeled masks by clicking foreground / background ob-\\nject points using a browser-based interactive segmentation\\ntool powered by SAM. Masks could be refined using pixel-\\nprecise “brush” and “eraser” tools. Our model-assisted an-\\nnotation runs in real-time directly inside a browser (using\\nprecomputed image embeddings) enabling a truly interac-\\ntive experience. We did not impose semantic constraints for\\nlabeling objects, and annotators freely labeled both “stuff”\\nand “things” [1]. We suggested annotators label objects\\nthey could name or describe, but did not collect these names\\nor descriptions. Annotators were asked to label objects in\\norder of prominence and were encouraged to proceed to the\\nnext image once a mask took over 30 seconds to annotate.\\n5At the start of this stage, SAM was trained using com-\\nmon public segmentation datasets. After sufficient data an-\\nnotation, SAM was retrained using only newly annotated\\nmasks. As more masks were collected, the image encoder\\nwas scaled from ViT-B to ViT-H and other architectural de-\\ntails evolved; in total we retrained our model 6 times. Av-\\nerage annotation time per mask decreased from 34 to 14\\nseconds as the model improved. We note that 14 seconds\\nis 6.5×faster than mask annotation for COCO [66] and\\nonly 2×slower than bounding-box labeling with extreme\\npoints [76, 71]. As SAM improved, the average number of\\nmasks per image increased from 20 to 44 masks. Overall,\\nwe collected 4.3M masks from 120k images in this stage.\\nSemi-automatic stage. In this stage, we aimed to increase\\nthediversity of masks in order to improve our model’s\\nability to segment anything. To focus annotators on less\\nprominent objects, we first automatically detected confident\\nmasks. Then we presented annotators with images prefilled\\nwith these masks and asked them to annotate any additional\\nunannotated objects. To detect confident masks, we trained\\na bounding box detector [84] on all first stage masks using a\\ngeneric “object” category. During this stage we collected an\\nadditional 5.9M masks in 180k images (for a total of 10.2M\\nmasks). As in the first stage, we periodically retrained our\\nmodel on newly collected data (5 times). Average annota-\\ntion time per mask went back up to 34 seconds (excluding\\nthe automatic masks) as these objects were more challeng-\\ning to label. The average number of masks per image went\\nfrom 44 to 72 masks (including the automatic masks).\\nFully automatic stage. In the final stage, annotation was\\nfully automatic . This was feasible due to two major en-\\nhancements to our model. First, at the start of this stage, we\\nhad collected enough masks to greatly improve the model,\\nincluding the diverse masks from the previous stage. Sec-\\nond, by this stage we had developed the ambiguity-aware\\nmodel, which allowed us to predict valid masks even in am-\\nbiguous cases. Specifically, we prompted the model with a\\n32×32 regular grid of points and for each point predicted\\na set of masks that may correspond to valid objects. With\\nthe ambiguity-aware model, if a point lies on a part or sub-\\npart, our model will return the subpart, part, and whole ob-\\nject. The IoU prediction module of our model is used to se-\\nlectconfident masks; moreover, we identified and selected\\nonly stable masks (we consider a mask stable if threshold-\\ning the probability map at 0.5−δand0.5 +δresults in\\nsimilar masks). Finally, after selecting the confident and\\nstable masks, we applied non-maximal suppression (NMS)\\nto filter duplicates. To further improve the quality of smaller\\nmasks, we also processed multiple overlapping zoomed-in\\nimage crops. For further details of this stage, see §B. We\\napplied fully automatic mask generation to all 11M images\\nin our dataset, producing a total of 1.1B high-quality masks.\\nWe describe and analyze the resulting dataset, SA-1B, next.\\nFigure 5: Image-size normalized mask center distributions.\\n5. Segment Anything Dataset\\nOur dataset, SA-1B, consists of 11M diverse, high-\\nresolution, licensed, and privacy protecting images and\\n1.1B high-quality segmentation masks collected with our\\ndata engine. We compare SA-1B with existing datasets\\nand analyze mask quality and properties. We are releasing\\nSA-1B to aid future development of foundation models for\\ncomputer vision. We note that SA-1B will be released un-\\nder a favorable license agreement for certain research uses\\nand with protections for researchers.\\nImages . We licensed a new set of 11M images from a\\nprovider that works directly with photographers. These im-\\nages are high resolution (3300 ×4950 pixels on average),\\nand the resulting data size can present accessibility and stor-'),\n",
       " Document(page_content='age challenges. Therefore, we are releasing downsampled\\nimages with their shortest side set to 1500 pixels. Even af-\\nter downsampling, our images are significantly higher reso-\\nlution than many existing vision datasets ( e.g., COCO [66]\\nimages are ∼480×640 pixels). Note that most models today\\noperate on much lower resolution inputs. Faces and vehicle\\nlicense plates have been blurred in the released images.\\nMasks . Our data engine produced 1.1B masks, 99.1% of\\nwhich were generated fully automatically. Therefore, the\\nquality of the automatic masks is centrally important. We\\ncompare them directly to professional annotations and look\\nat how various mask properties compare to prominent seg-\\nmentation datasets. Our main conclusion, as borne out in\\nthe analysis below and the experiments in §7, is that our\\nautomatic masks are high quality and effective for training\\nmodels. Motivated by these findings, SA-1B only includes\\nautomatically generated masks.\\nMask quality. To estimate mask quality, we randomly sam-\\npled 500 images ( ∼50k masks) and asked our professional\\nannotators to improve the quality of all masks in these im-\\nages. Annotators did so using our model and pixel-precise\\n“brush” and “eraser” editing tools. This procedure resulted\\nin pairs of automatically predicted and professionally cor-\\nrected masks. We computed IoU between each pair and\\nfound that 94% of pairs have greater than 90% IoU (and\\n97% of pairs have greater than 75% IoU). For comparison,\\nprior work estimates inter-annotator consistency at 85-91%\\nIoU [44, 60]. Our experiments in §7 confirm by human rat-\\nings that mask quality is high relative to a variety of datasets\\nand that training our model on automatic masks is nearly as\\ngood as using all masks produced by the data engine.\\n6SA-1B\\n11M images\\n1129M (1.1B) masksLVIS v1\\n0.120M images\\n1.5M masksCOCO\\n0.123M images\\n0.9M masksADE20K\\n0.028M images\\n0.7M masksOpen Images\\n1M images\\n2.7M masks\\n<10 11-50 51-100 101-200 >200\\nNumber of masks per image04080Percent of images\\n0.00 0.25 0.50 0.75\\nRelative segmentation mask size100\\n10−2Percent of masks\\n0.0 0.2 0.4 0.6 0.8\\nConcavity051015Percent of masksFigure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B\\nhas 11×more images and 400 ×more masks than the largest existing segmentation dataset Open Images [60].\\nPer country\\nimage count\\n≥ 100k\\n< 100k\\n< 10k\\n< 1k\\nRUS\\nTHA\\nUSA\\nITA\\nGBR\\nDEU\\nESP\\nIDN\\nUKR\\nFRA\\nJPN\\nMYS\\nTUR\\nIND\\nCHN\\nPOL\\nNLD\\nVNM\\nBRA\\nCAN\\nGRC\\nAUS\\nPRT\\nCZE\\nBLR\\nROU\\nKOR\\nARE\\nAUT\\nSWE\\nTWN\\nHKG\\nCHE\\nISR\\nSGP\\nHUN\\nBEL\\nHRV\\nBGR\\nPHL\\nKAZ\\nMEX\\nNOR\\nMMR\\nZAF\\nSRB\\nDNK\\nMAR\\nFIN\\nLVA\\n50 most common countries (ISO codes)0200k400k600k800kNumber of images per countryAsia & Oceania\\nAfrica\\nEurope\\nNorth America\\nLatin America & Caribbean\\nFigure 7: Estimated geographic distribution of SA-1B images. Most of the world’s countries have more than 1000 images in\\nSA-1B, and the three countries with the most images are from different parts of the world.\\nMask properties. In Fig. 5 we plot the spatial distribution\\nof object centers in SA-1B compared to the largest existing\\nsegmentation datasets. Common photographer biases are\\npresent in all datasets. We observe that SA-1B has greater\\ncoverage of image corners compared to LVIS v1 [44] and\\nADE20K [117], the two most similarly distributed datasets,\\nwhile COCO [66] and Open Images V5 [60] have a more\\nprominent center bias. In Fig. 6 (legend) we compare these\\ndatasets by size. SA-1B has 11 ×more images and 400 ×\\nmore masks than the second largest, Open Images. On av-\\nerage, it has 36 ×more masks per image than Open Images.\\nThe closest dataset in this respect, ADE20K, still has 3.5 ×\\nfewer masks per image. Fig. 6 (left) plots the masks-per-\\nimage distribution. Next, we look at image-relative mask\\nsize (square root of the mask area divided by image area)\\nin Fig. 6 (middle). As expected, since our dataset has more\\nmasks per image, it also tends to include a greater percent-\\nage of small and medium relative-size masks. Finally, to\\nanalyze shape complexity, we look at mask concavity (1\\nminus mask area divided by area of mask’s convex hull) in\\nFig. 6 (right). Since shape complexity is correlated with\\nmask size, we control for the datasets’ mask size distribu-\\ntions by first performing stratified sampling from binned\\nmask sizes. We observe that the concavity distribution of\\nour masks is broadly similar to that of other datasets.\\n6. Segment Anything RAI Analysis\\nWe next perform a Responsible AI (RAI) analysis of our\\nwork by investigating potential fairness concerns and bi-\\nases when using SA-1B and SAM. We focus on the geo-\\ngraphic and income distribution of SA-1B and fairness of\\nSAM across protected attributes of people. We also provide\\ndataset, data annotation, and model cards in §F.SA-1B % images\\n# countries #imgs #masks SA-1B COCO O.I.\\nAfrica 54 300k 28M 2.8% 3.0% 1.7%\\nAsia & Oceania 70 3.9M 423M 36.2% 11.4% 14.3%\\nEurope 47 5.4M 540M 49.8% 34.2% 36.2%\\nLatin America & Carib. 42 380k 36M 3.5% 3.1% 5.0%\\nNorth America 4830k 80M 7.7% 48.3% 42.8%\\nhigh income countries 81 5.8M 598M 54.0% 89.1% 87.5%\\nmiddle income countries 108 4.9M 499M 45.0% 10.5% 12.0%\\nlow income countries 28 100k 9.4M 0.9% 0.4% 0.5%\\nTable 1: Comparison of geographic and income representa-\\ntion. SA-1B has higher representation in Europe and Asia &\\nOceania as well as middle income countries. Images from\\nAfrica, Latin America & Caribbean, as well as low income\\ncountries, are underrepresented in all datasets.\\nGeographic and income representation. We infer the\\ncountry images were photographed in using standard meth-\\nods (see §C). In Fig. 7 we visualize the per-country image\\ncounts in SA-1B (left) and the 50 countries with the most\\nimages (right). We note that the top-three countries are\\nfrom different parts of the world. Next, in Table 1 we com-\\npare the geographic and income representation of SA-1B,\\nCOCO [66], and Open Images [60]. SA-1B has a substan-\\ntially higher percentage of images in Europe and Asia &\\nOceania as well as in middle income countries. All datasets\\nunderrepresent Africa as well as low income countries. We\\nnote that in SA-1B, all regions, including Africa, have at\\nleast 28 million masks, 10 ×more than the total number of\\nmasks of any previous dataset. Finally, we observe that the\\naverage number of masks per image (not shown) is fairly\\nconsistent across region and income (94-108 per image).\\n7mIoU at\\n1 point 3 points\\nperceived gender presentation\\nfeminine 54.4 ±1.7 90.4±0.6\\nmasculine 55.7 ±1.7 90.1±0.6\\nperceived age group\\nolder 62.9 ±6.7 92.6±1.3\\nmiddle 54.5 ±1.3 90.2±0.5\\nyoung 54.2 ±2.2 91.2±0.7mIoU at\\n1 point 3 points\\nperceived skin tone\\n1 52.9 ±2.2 91.0±0.9\\n2 51.5 ±1.4 91.1±0.5\\n3 52.2 ±1.9 91.4±0.7\\n4 51.5 ±2.7 91.7±1.0\\n5 52.4 ±4.2 92.5±1.4\\n6 56.7 ±6.3 91.2±2.4\\nTable 2: SAM’s performance segmenting people across per-\\nceived gender presentation, age group, and skin tone. 95%\\nconfidence intervals are shown. Within each grouping, all\\nconfidence intervals overlap except older vs. middle.\\nFairness in segmenting people. We investigate potential\\nfairness concerns across perceived gender presentation, per-\\nceived age group, and perceived skin tone by measuring\\nthe performance discrepancy of SAM between groups. We\\nuse the More Inclusive Annotations for People (MIAP) [87]\\ndataset for gender presentation and age and a proprietary\\ndataset for skin tone (see §C). Our evaluation uses simu-\\nlated interactive segmentation with random sampling of 1\\nand 3 points (see §D). Table 2 (top left) shows results for\\nperceived gender presentation. We note that females have\\nbeen shown to be underrepresented in detection and seg-\\nmentation datasets [115], but observe that SAM performs\\nsimilarly across groups. We repeat the analysis for per-\\nceived age in Table 2 (bottom left), noting that those who\\nare perceived to be younger and older have been shown to\\nbe underrepresented in large-scale datasets [110]. SAM per-\\nforms best on those who are perceived older (although the\\nconfidence interval is large). Finally, we repeat the anal-\\nysis for perceived skin tone in Table 2 (right), noting that\\nthose with lighter apparent skin tones have been shown to\\nbe overrepresented and those with darker skin tones under-\\nrepresented in large-scale datasets [110]. As MIAP does\\nnot contain perceived skin tone annotations, we use a pro-\\nprietary dataset that contains annotations for the perceived\\nFitzpatrick skin type [36], which ranges from 1 (lightest\\nskin tone) to 6 (darkest skin tone). While the means vary\\nsomewhat, we do not find a significant difference across\\ngroups. We believe our findings stem from the nature of\\nthe task, and acknowledge biases may arise when SAM is\\nused as a component in larger systems. Finally, in §C we\\nextend the analysis to segmenting clothing where we find\\nan indication of bias across perceived gender presentation.\\n7. Zero-Shot Transfer Experiments\\nIn this section, we present zero-shot transfer experiments\\nwith SAM, the Segment Anything Model. We consider five\\ntasks, four of which differ significantly from the promptable\\nsegmentation task used to train SAM. These experiments\\nevaluate SAM on datasets and tasks that were not seen dur-ing training (our usage of “zero-shot transfer” follows its\\nusage in CLIP [82]). The datasets may include novel image\\ndistributions, such as underwater or ego-centric images ( e.g.\\nFig. 8) that, to our knowledge, do not appear in SA-1B.\\nOur experiments begin by testing the core goal of\\npromptable segmentation: producing a valid mask from any\\nprompt. We emphasize the challenging scenario of a single\\nforeground point prompt, since it is more likely to be am-\\nbiguous than other more specific prompts. Next, we present\\na sequence of experiments that traverse low, mid, and high-\\nlevel image understanding and roughly parallel the histori-\\ncal development of the field. Specifically, we prompt SAM\\nto (1) perform edge detection, (2) segment everything, i.e.\\nobject proposal generation, (3) segment detected objects,\\ni.e. instance segmentation, and (4), as a proof-of-concept, to\\nsegment objects from free-form text. These four tasks dif-\\nfer significantly from the promptable segmentation task that\\nSAM was trained on and are implemented via prompt engi-\\nneering. Our experiments conclude with an ablation study.\\nImplementation. Unless otherwise specified: (1) SAM\\nuses an MAE [47] pre-trained ViT-H [33] image encoder\\nand (2) SAM was trained on SA-1B, noting that this dataset\\nincludes only automatically generated masks from the final\\nstage of our data engine. For all other model and training\\ndetails, such as hyperparameters, refer to §A.\\n7.1. Zero-Shot Single Point Valid Mask Evaluation\\nTask. We evaluate segmenting an object from a single fore-\\nground point. This task is ill-posed as one point can refer\\nto multiple objects. Ground truth masks in most datasets\\ndo not enumerate allpossible masks, which can make au-\\ntomatic metrics unreliable. Therefore, we supplement the\\nstandard mIoU metric ( i.e., the mean of all IoUs between\\npredicted and ground truth masks) with a human study in\\nwhich annotators rate mask quality from 1 (nonsense) to 10\\n(pixel-perfect). See §D.1, §E, and §G for additional details.\\nBy default, we sample points from the “center” of ground\\ntruth masks (at a maximal value of the mask’s interior dis-\\ntance transform), following the standard evaluation proto-\\ncol in interactive segmentation [92]. Since SAM is capable\\nof predicting multiple masks, we evaluate only the model’s\\nmost confident mask by default. The baselines are all\\nsingle-mask methods. We compare mainly to RITM [92],\\na strong interactive segmenter that performs best on our\\nbenchmark compared to other strong baselines [67, 18].\\nDatasets. We use a newly compiled suite of 23 datasets\\nwith diverse image distributions. Fig. 8 lists the datasets\\nand shows a sample from each one (see appendix Table 7 for\\nmore details). We use all 23 datasets for mIoU evaluation.\\nFor the human study, we use the subset listed in Fig. 9b'),\n",
       " Document(page_content='(due to the resource requirements of such studies). This\\nsubset includes both datasets for which SAM outperforms\\nand underperforms RITM according to automatic metrics.\\n8ADE20K [117] BBBC038v1 [12] Cityscapes [25] DOORS [80] DRAM [24] EgoHOS [113] GTEA [34, 63] Hypersim [86]\\nIBD [17] iShape [111] LVIS [44] NDD20 [100] NDISPark [22, 23] OVIS [81] PPDLS [74] Plittersdorf [46]\\nSTREETS [91] TimberSeg [38] TrashCan [52] VISOR [28, 27] WoodScape [112] PIDRay [104] ZeroWaste-f [6]\\nFigure 8: Samples from the 23 diverse segmentation datasets used to evaluate SAM’s zero-shot transfer capabilities.\\n-20 0 +20 +40\\nIoU delta at 1 center pointGTEA [34, 63]TrashCan [52]DRAM [24]PIDRay [104]Cityscapes [25]WoodScape [112]IBD [17]EgoHOS [113]Plittersdorf [46]VISOR [28, 27]NDISPark [22, 23]Hypersim [86]OVIS [81]ADE20K [117]iShape [111]ZeroWaste-f [6]STREETS [91]LVIS [44]NDD20 [100]TimberSeg [38]DOORS [80]BBBC038v1 [12]PPDLS [74]\\n-21.4-15.0-6.5-5.8-2.0-0.6-0.3+0.8+1.5+1.8+2.7+6.1+7.0+7.8+8.8+9.1+17.3+18.5+21.1+28.9+41.1+44.7+46.9\\n(a) SAM vs. RITM [92] on 23 datasets\\nLVIS VISOR DRAM IBD NDD20 OVIS iShape\\nDatasets579Avg. mask ratingGround Truth\\nSAM\\nSAM - single output\\nRITM\\n(b) Mask quality ratings by human annotators\\n123 5 9\\nNumber of points5075mIoU (23 datasets)\\nSAM (oracle)\\nSAM\\nRITM\\nSimpleClick\\nFocalClick\\n123 5 9\\nNumber of points5075mIoU (23 datasets)\\nSAM (oracle)\\n(c) Center points (default) (d) Random points\\nFigure 9: Point to mask evaluation on 23 datasets. (a) Mean IoU of SAM and the strongest single point segmenter, RITM [92].\\nDue to ambiguity, a single mask may not match ground truth; circles show “oracle” results of the most relevant of SAM’s 3\\npredictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use\\nthe ground truth mask center as the prompt. (c, d) mIoU with varying number of points. SAM significantly outperforms prior\\ninteractive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.\\nResults. First, we look at automatic evaluation on the full\\nsuite of 23 datasets using mIoU. We compare per-dataset\\nresults in Fig. 9a against RITM. SAM yields higher re-\\nsults on 16 of the 23 datasets, by as much as ∼47 IoU. We\\nalso present an “oracle” result, in which the most relevant\\nof SAM’s 3 masks is selected by comparing them to the\\nground truth, rather than selecting the most confident mask.\\nThis reveals the impact of ambiguity on automatic evalu-\\nation. In particular, with the oracle to perform ambiguity\\nresolution, SAM outperforms RITM on alldatasets.\\nResults of the human study are presented in Fig. 9b. Er-\\nror bars are 95% confidence intervals for mean mask rat-\\nings (all differences are significant; see §E for details). We\\nobserve that the annotators consistently rate the quality of\\nSAM’s masks substantially higher than the strongest base-\\nline, RITM. An ablated, “ambiguity-unaware” version of\\nSAM with a single output mask has consistently lower rat-\\nings, though still higher than RITM. SAM’s mean ratingsfall between 7 and 9, which corresponds to the qualitative\\nrating guideline: “ A high score (7-9): The object is identi-\\nfiable and errors are small and rare ( e.g., missing a small,\\nheavily obscured disconnected component, ...). ” These re-\\nsults indicate that SAM has learned to segment valid masks\\nfrom a single point. Note that for datasets like DRAM and\\nIBD, where SAM is worse on automatic metrics, it receives\\nconsistently higher ratings in the human study .\\nFig. 9c shows additional baselines, SimpleClick [67] and\\nFocalClick [18], which obtain lower single point perfor-\\nmance than RITM and SAM. As the number of points in-\\ncreases from 1 to 9, we observe that the gap between meth-\\nods decreases. This is expected as the task becomes easier;\\nalso, SAM is not optimized for the very high IoU regime.\\nFinally, in Fig. 9d we replace the default center point sam-\\npling with random point sampling. We observe that the gap\\nbetween SAM and the baselines grows and SAM is able to\\nachieve comparable results under either sampling method.\\n9image ground truth SAM\\nFigure 10: Zero-shot edge prediction on BSDS500. SAM\\nwas not trained to predict edge maps nor did it have access\\nto BSDS images or annotations during training.\\nmethod year ODS OIS AP R50\\nHED [108] 2015 .788 .808 .840 .923\\nEDETR [79] 2022 .840 .858 .896 .930\\nzero-shot transfer methods:\\nSobel filter 1968 .539 - - -\\nCanny [13] 1986 .600 .640 .580 -\\nFelz-Hutt [35] 2004 .610 .640 .560 -\\nSAM 2023 .768 .786 .794 .928\\nTable 3: Zero-shot transfer to edge detection on BSDS500.\\n7.2. Zero-Shot Edge Detection\\nApproach. We evaluate SAM on the classic low-level task\\nof edge detection using BSDS500 [72, 3]. We use a sim-\\nplified version of our automatic mask generation pipeline.\\nSpecifically, we prompt SAM with a 16 ×16 regular grid of\\nforeground points resulting in 768 predicted masks (3 per\\npoint). Redundant masks are removed by NMS. Then, edge\\nmaps are computed using Sobel filtering of unthresholded\\nmask probability maps and standard lightweight postpro-\\ncessing, including edge NMS (see §D.2 for details).\\nResults. We visualize representative edge maps in Fig. 10\\n(see Fig. 15 for more). Qualitatively, we observe that even\\nthough SAM was not trained for edge detection, it produces\\nreasonable edge maps. Compared to the ground truth, SAM\\npredicts more edges, including sensible ones that are not an-\\nnotated in BSDS500. This bias is reflected quantitatively in\\nTable 3: recall at 50% precision (R50) is high, at the cost of\\nprecision. SAM naturally lags behind state-of-the-art meth-\\nods that learn the biases of BSDS500, i.e., which edges to\\nsuppress. Nevertheless, SAM performs well compared to\\npioneering deep learning methods such as HED [108] (also\\ntrained on BSDS500) and significantly better than prior,\\nthough admittedly outdated, zero-shot transfer methods.\\n7.3. Zero-Shot Object Proposals\\nApproach. Next, we evaluate SAM on the mid-level task\\nof object proposal generation [2, 102]. This task has played\\nan important role in object detection research, serving as anmask AR@1000\\nmethod all small med. large freq. com. rare\\nViTDet-H [62] 63.0 51.7 80.8 87.0 63.1 63.3 58.3\\nzero-shot transfer methods:\\nSAM – single out. 54.9 42.8 76.7 74.4 54.7 59.8 62.0\\nSAM 59.3 45.5 81.6 86.9 59.1 63.9 65.8\\nTable 4: Object proposal generation on LVIS v1. SAM is\\napplied zero-shot, i.e. it was not trained for object proposal\\ngeneration nor did it access LVIS images or annotations.\\nintermediate step in pioneering systems ( e.g., [102, 41, 84]).\\nTo generate object proposals, we run a slightly modified\\nversion of our automatic mask generation pipeline and out-\\nput the masks as proposals (see §D.3 for details).\\nWe compute the standard average recall (AR) metric on\\nLVIS v1 [44]. We focus on LVIS because its large number\\nof categories presents a challenging test. We compare to\\nastrong baseline implemented as a ViTDet [62] detector\\n(with cascade Mask R-CNN [48, 11] ViT-H). We note that\\nthis “baseline” corresponds to the “Detector Masquerading\\nas Proposal generator” (DMP) method [16] that was shown\\nto game AR, making it a truly demanding comparison.\\nResults. In Table 4 we see unsurprisingly that using the\\ndetections from ViTDet-H as object proposals ( i.e., the\\nDMP method [16] that games AR) performs the best over-\\nall. However, SAM does remarkably well on several met-\\nrics. Notably, it outperforms ViTDet-H on medium and\\nlarge objects, as well as rare and common objects. In fact,\\nSAM only underperforms ViTDet-H on small objects and\\nfrequent objects, where ViTDet-H can easily learn LVIS-\\nspecific annotation biases since it was trained on LVIS, un-\\nlike SAM. We also compare against an ablated ambiguity-\\nunaware version of SAM (“single out.”), which performs\\nsignificantly worse than SAM on all AR metrics.\\n7.4. Zero-Shot Instance Segmentation\\nApproach. Moving to higher-level vision, we use SAM\\nas the segmentation module of an instance segmenter. The\\nimplementation is simple: we run a object detector (the\\nViTDet used before) and prompt SAM with its output\\nboxes. This illustrates composing SAM in a larger system.\\nResults. We compare the masks predicted by SAM and\\nViTDet on COCO and LVIS in Table 5. Looking at the\\nmask AP metric we observe gaps on both datasets, where\\nSAM is reasonably close, though certainly behind ViTDet.\\nBy visualizing outputs, we observed that SAM masks are\\noften qualitatively better than those of ViTDet, with crisper\\nboundaries (see §D.4 and Fig. 16). To investigate this ob-\\nservation, we conducted an additional human study asking\\nannotators to rate the ViTDet masks and SAM masks on the\\n1 to 10 quality scale used before. In Fig. 11 we observe that\\nSAM consistently outperforms ViTDet in the human study.\\n10COCO [66] LVIS v1 [44]\\nmethod AP APSAPMAPLAP APSAPMAPL\\nViTDet-H [62] 51.0 32.0 54.3 68.9 46.6 35.0 58.0 66.3\\nzero-shot transfer methods (segmentation module only):\\nSAM 46.5 30.8 51.0 61.7 44.7 32.5 57.6 65.5\\nTable 5: Instance segmentation results. SAM is prompted\\nwith ViTDet boxes to do zero-shot segmentation. The fully-\\nsupervised ViTDet outperforms SAM, but the gap shrinks\\non the higher-quality LVIS masks. Interestingly, SAM out-\\nperforms ViTDet according to human ratings (see Fig. 11).\\n1 2 3 4 5 6 7 8 9 10\\nMask quality rating02040Percent of ratings8.6 ± 0.06, LVIS GT\\n8.1 ± 0.07, SAM\\n7.9 ± 0.08, ViTDet-H\\n7.6 ± 0.12, COCO GT\\nFigure 11: Mask quality rating distribution from our human\\nstudy for ViTDet and SAM, both applied to LVIS ground\\ntruth boxes. We also report LVIS and COCO ground truth\\nquality. The legend shows rating means and 95% confi-\\ndence intervals. Despite its lower AP (Table 5), SAM has\\nhigher ratings than ViTDet, suggesting that ViTDet exploits\\nbiases in the COCO and LVIS training data.\\nWe hypothesize that on COCO, where the mask AP gap\\nis larger and the ground truth quality is relatively low (as\\nborne out by the human study), ViTDet learns the specific\\nbiases of COCO masks. SAM, being a zero-shot method,\\nis unable to exploit these (generally undesirable) biases.\\nThe LVIS dataset has higher quality ground truth, but there\\nare still specific idiosyncrasies ( e.g., masks do not contain\\nholes, they are simple polygons by construction) and biases\\nfor modal vs. amodal masks. Again, SAM is not trained to\\nlearn these biases, while ViTDet can exploit them.\\n7.5. Zero-Shot Text-to-Mask\\nApproach. Finally, we consider an even higher-level task:\\nsegmenting objects from free-form text. This experiment\\nis a proof-of-concept of SAM’s ability to process text\\nprompts. While we used the exact same SAM in all prior\\nexperiments, for this one SAM’s training procedure is mod-\\nified to make it text-aware, but in a way that does not require\\nnew text annotations. Specifically, for each manually col-\\nlected mask with area larger than 1002we extract the CLIP\\nimage embedding. Then, during training, we prompt SAM\\nwith the extracted CLIP image embeddings as its first in-\\nteraction. The key observation here is that because CLIP’s\\nimage embeddings are trained to align with its textembed-\\ndings, we can train with image embeddings, but use text\\nembeddings for inference. That is, at inference time we run\\ntext through CLIP’s text encoder and then give the resulting\\ntext embedding as a prompt to SAM (see §D.5 for details).\\n     “a wheel”\\n✓\\n     “beaver tooth grille” ✓\\n     “a wiper”\\n✗\\n     “a wiper” + point\\n ✓\\n     “wipers”\\n✗\\n     “wipers” + point\\n ✓\\nFigure 12: Zero-shot text-to-mask. SAM can work with\\nsimple and nuanced text prompts. When SAM fails to make\\na correct prediction, an additional point prompt can help.\\nResults. We show qualitative results in Fig. 12. SAM\\ncan segment objects based on simple text prompts like “a\\nwheel” as well as phrases like “beaver tooth grille”. When\\nSAM fails to pick the right object from a text prompt only,\\nan additional point often fixes the prediction, similar to [31].\\n7.6. Ablations\\nWe perform several ablations on our 23 dataset suite with')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summaries of the document using map reduce method \n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Summarization chain\n",
    "summary_prompt = \"\"\"\n",
    "You are an expert in the field of meta-learning. \n",
    "You are provided with a text. \n",
    "You need to extract the key points from the text. \n",
    "The points should be helpful in creating a script for a podcast episode.\n",
    "\n",
    "\n",
    "TEXT: {text}\n",
    "\n",
    "Key points:\n",
    "\"\"\"\n",
    "map_prompt_template = PromptTemplate(input_variables=[\"text\"],\n",
    "                                template=summary_prompt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combining all the summaries together \n",
    "combine_prompt = \"\"\"\n",
    "Provide a final summary of the entire document with these key points \n",
    "The overall summary should be helpful in creating a script for a podcast episode.\n",
    "When dealing with scientific topics, mention the data as well.\n",
    "\n",
    "Key points: {key_points}\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate(input_variables=[\"key_points\"],\n",
    "                                template=combine_prompt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summary_chain = load_summarize_chain(\n",
    "    llm = llm, \n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt_template,\n",
    "    map_reduce_document_variable_name=\"text\",\n",
    "    combine_prompt=reduce_prompt_template,\n",
    "    combine_document_variable_name=\"key_points\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "output = summary_chain.invoke(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this podcast episode, we explored the Segment Anything project and the Segment Anything Model (SAM) for image segmentation. SAM is designed to be promptable, allowing for zero-shot transfer to new tasks and distributions. The SA-1B dataset, with over 1 billion masks, showcases SAM's capabilities. SAM has shown promising results in various tasks but still has some limitations. \\n\\nWe also discussed the importance of evaluating mask quality in image segmentation, highlighting the impact of errors on the final score. Consistency in judgment and adherence to rating criteria are essential for accurate evaluations. Understanding and improving mask quality is crucial in image segmentation tasks. \\n\\nOverall, the Segment Anything project and SAM model represent significant advancements in image segmentation, offering a powerful tool for prompt-based segmentation tasks and zero-shot transfer learning in computer vision applications.\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model=MODEL)\n",
    "\n",
    "# Podcast chain \n",
    "podcast_writer = \"\"\"\n",
    "System: You are an expert scriptwriter. You are provided with a summary of a document.\n",
    "You need to create a coherent and engaging script for a podcast episode based on this summary.\n",
    "Make it a conversation between two hosts.\n",
    "Remember, the podcast is for a {AUDIENCE} audience. \n",
    "There should at least be 10 dialogues from each person. Make it detailed and as a conversation\n",
    "Please follow the structure below for the script for a 2 min episode.:\n",
    "\n",
    "Summary: {summary}\n",
    "\n",
    "Podcast Script:\n",
    "Person 1: \n",
    "Person 2:\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "writer_prompt = PromptTemplate(input_variables=[\"AUDIENCE\", \"summary\"],\n",
    "                                template=podcast_writer)\n",
    "writer_chain = writer_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = writer_chain.invoke({\"AUDIENCE\": AUDIENCE, \n",
    "                                      \"summary\": output['output_text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Person 1',\n",
       "  \" Welcome back to our podcast, where we dive deep into the world of cutting-edge technology. Today, we're going to explore the fascinating Segment Anything project and the Segment Anything Model, also known as SAM.\"),\n",
       " ('Person 2',\n",
       "  \" That's right! SAM is truly groundbreaking with its promptable design that allows for zero-shot transfer to new tasks and distributions. It's amazing to see how it showcases such strong performance in various downstream tasks.\"),\n",
       " ('Person 1',\n",
       "  \" Absolutely! And let's not forget about the SA-1B dataset for image segmentation, which contains over 1 billion masks on 11 million images. It's a treasure trove of valuable data collected through a model-assisted data collection process.\"),\n",
       " ('Person 2',\n",
       "  \" The sheer scale of the SA-1B dataset is mind-blowing. It's definitely a game-changer for researchers looking to push the boundaries of image segmentation.\"),\n",
       " ('Person 1',\n",
       "  \" And speaking of image segmentation, we also delved into the world of image annotation in this episode. It's crucial to have clear guidelines, training, and continuous improvement to enhance annotator performance and annotation quality.\")]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = []\n",
    "for dialogue in final_response.content.split(\"\\n\\n\"):\n",
    "    person, text = dialogue.split(':')\n",
    "    conversation.append((person, text))\n",
    "\n",
    "conversation[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import play, save\n",
    "from elevenlabs.client import ElevenLabs\n",
    "\n",
    "\n",
    "el_api_key = os.getenv(\"ELEVEN_LABS_API_KEY\")\n",
    "client = ElevenLabs(api_key=el_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = client.voices.get_all()\n",
    "len(responses.voices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for i, value in enumerate(conversation):\n",
    "    speaker, line = value\n",
    "    if speaker == \"Person 1\":\n",
    "        voice = \"Chris\"\n",
    "    else:\n",
    "        voice = \"Gigi\"\n",
    "    audio = client.generate(text=line, voice=voice, model='eleven_monolingual_v1')\n",
    "    play(audio)\n",
    "    audios.append([i, voice, audio])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"audio_files\", exist_ok=True)\n",
    "for i, voice, audio in audios:\n",
    "    save(audio, f\"audio_files/{voice}_{i}.mp3\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump refine outputs into a file\n",
    "import json\n",
    "with open(\"refine_outputs.json\", \"w\") as f:\n",
    "    json.dump(refine_outputs['output_text'], f)\n",
    "\n",
    "# Load refine outputs from a file\n",
    "import json\n",
    "with open(\"refine_outputs.json\", \"r\") as f:\n",
    "    refine_outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Segment Anything Model (SAM) is a decoder model developed by the FAIR team of Meta AI for prompt-based segmentation tasks. It predicts multiple masks using various loss functions and incorporates geographic information using named entity recognition and APIs. SAM has been evaluated for segmenting clothing by gender and age, zero-shot transfer capabilities, edge detection, object proposals, and text-to-mask modeling. A variation of SAM, known as SAM, produces a single mask per image with similar performance. The dataset used, Segment Anything 1B (SA-1B), is one of the most geographically diverse segmentation datasets and is intended for research purposes only. Annotators were trained and compensated using a proprietary platform, and the dataset will be released under a license agreement for specific research purposes. SAM has impressive zero-shot performance but may miss fine structures and hallucinate small disconnected components. It was trained on licensed images, and the environmental impact of training the model was considered. SAM is intended for research purposes and should be used with caution for zero-shot segmentation tasks. Users are recommended to run their fairness evaluation and exercise judgment for downstream use of the model. The model's quality can be assessed through an interface that provides different views of the mask for evaluation, allowing users to rate the quality based on specific criteria. Additionally, guidelines have been provided for annotators to review mask quality, including assessing boundary accuracy, correspondence to provided points or boxes, and avoiding errors such as including arbitrary parts of collections or missing parts of objects. Mask scoring guidelines have been provided to help users evaluate the quality of the masks produced by SAM. The guidelines include examples of masks with different scores, ranging from medium-to-high scores to very high scores, to assist in the evaluation process.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model=MODEL)\n",
    "\n",
    "# Podcast chain \n",
    "podcast_writer = \"\"\"\n",
    "System: You are an expert scriptwriter. You are provided with a summary of a document.\n",
    "You need to create a coherent and engaging script for a podcast episode based on this summary.\n",
    "Make it a conversation between two hosts.\n",
    "Remember, the podcast is for a {AUDIENCE} audience. \n",
    "Please follow the structure below for the script for a 2 min episode.:\n",
    "\n",
    "Summary: {summary}\n",
    "\n",
    "Podcast Script:\n",
    "Person 1: \n",
    "Person 2:\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "writer_prompt = PromptTemplate(input_variables=[\"AUDIENCE\", \"summary\"],\n",
    "                                template=podcast_writer)\n",
    "writer_chain = writer_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = writer_chain.invoke({\"AUDIENCE\": AUDIENCE, \n",
    "                                      \"summary\": refine_outputs})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Person 1: Welcome back to our podcast, where we explore the latest advancements in AI technology. Today, we're diving into the world of segmentation models with the Segment Anything Model, or SAM, developed by the FAIR team of Meta AI.\\n\\nPerson 2: That's right! SAM is a decoder model that is specifically designed for prompt-based segmentation tasks. It's pretty impressive how SAM predicts multiple masks using different loss functions and incorporates geographic information using named entity recognition and APIs.\\n\\nPerson 1: Absolutely. SAM has been evaluated for various tasks such as segmenting clothing by gender and age, edge detection, and even text-to-mask modeling. It's fascinating to see how this model can be used in different applications.\\n\\nPerson 2: And let's not forget about SAM+, a variation of SAM that produces a single mask per image with similar performance. The dataset used, Segment Anything 1B (SA-1B), is one of the most geographically diverse segmentation datasets out there.\\n\\nPerson 1: It's important to note that SAM has impressive zero-shot performance, but there are some limitations, such as missing fine structures and hallucinating small disconnected components. It's crucial to exercise caution when using SAM for zero-shot segmentation tasks.\\n\\nPerson 2: Definitely. Users are encouraged to run fairness evaluations and exercise judgment when using SAM for downstream tasks. The model's quality can be assessed through an interface that provides different views of the mask for evaluation based on specific criteria.\\n\\nPerson 1: And let's not forget about the guidelines provided for annotators to review mask quality, including assessing boundary accuracy and avoiding errors. These guidelines are essential for ensuring the accuracy of the masks produced by SAM.\\n\\nPerson 2: Overall, SAM is a powerful tool for segmentation tasks, but it's essential to use it responsibly and with caution. Make sure to check out the guidelines provided to assess the quality of the masks and make informed decisions when using SAM for your research purposes.\\n\\nPerson 1: That's all the time we have for today. Thanks for tuning in, and we'll catch you on the next episode of our podcast. Stay curious and keep exploring the world of AI technology.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Person 1: Welcome back to our podcast, where we explore the latest advancements in AI technology. Today, we're diving into the world of segmentation models with the Segment Anything Model, or SAM, developed by the FAIR team of Meta AI.\\n\\nPerson 2: That's right! SAM is a decoder model that is specifically designed for prompt-based segmentation tasks. It's pretty impressive how SAM predicts multiple masks using different loss functions and incorporates geographic information using named entity recognition and APIs.\\n\\nPerson 1: Absolutely. SAM has been evaluated for various tasks such as segmenting clothing by gender and age, edge detection, and even text-to-mask modeling. It's fascinating to see how this model can be used in different applications.\\n\\nPerson 2: And let's not forget about SAM+, a variation of SAM that produces a single mask per image with similar performance. The dataset used, Segment Anything 1B (SA-1B), is one of the most geographically diverse segmentation datasets out there.\\n\\nPerson 1: It's important to note that SAM has impressive zero-shot performance, but there are some limitations, such as missing fine structures and hallucinating small disconnected components. It's crucial to exercise caution when using SAM for zero-shot segmentation tasks.\\n\\nPerson 2: Definitely. Users are encouraged to run fairness evaluations and exercise judgment when using SAM for downstream tasks. The model's quality can be assessed through an interface that provides different views of the mask for evaluation based on specific criteria.\\n\\nPerson 1: And let's not forget about the guidelines provided for annotators to review mask quality, including assessing boundary accuracy and avoiding errors. These guidelines are essential for ensuring the accuracy of the masks produced by SAM.\\n\\nPerson 2: Overall, SAM is a powerful tool for segmentation tasks, but it's essential to use it responsibly and with caution. Make sure to check out the guidelines provided to assess the quality of the masks and make informed decisions when using SAM for your research purposes.\\n\\nPerson 1: That's all the time we have for today. Thanks for tuning in, and we'll catch you on the next episode of our podcast. Stay curious and keep exploring the world of AI technology.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
